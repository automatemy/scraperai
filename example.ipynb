{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## How to scrape any website with ScraperAI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d597f324e60cad72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before we start, install the package"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcd4546522a05bc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ! pip install scraperai"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76211611f93937da",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scraperai.models import WebpageFields, Pagination, WebpageType, ScraperConfig\n",
    "from scraperai import ParserAI, Scraper\n",
    "from scraperai.crawlers import SeleniumCrawler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1103d896f48fa663",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. Init crawler\n",
    "\n",
    "First, we need to initialize a web-crawler that will help us to fetch data from the web.\n",
    "\n",
    "In this tutorial we use `SeleniumCrawler` that uses Selenium webdriver. By default it creates a new Chrome session.\n",
    "\n",
    "To use other browsers you can pass your own webdriver (both local and remote) to the `SeleniumCrawler`:\n",
    "```\n",
    "crawler = SeleniumCrawler(driver=your_own_webdriver)\n",
    "```\n",
    "\n",
    "If you want to use playwright or other services, you can create your own crawler implementation based on `BaseCrawler`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1180076edb7afa4a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crawler = SeleniumCrawler()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "456b1481d75ae268",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Init ParserAI\n",
    "\n",
    "By default, we use the latest OpenAI GPT-4 model. You can place your API key in the `.env` file. If you don't have a key, you can get it [here](https://platform.openai.com/api-keys).\n",
    "Also, you can use another AI model. To do this, you need to create another implementations of the `BaseJsonLM` and `BaseVision` classes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17011078f7add63"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env_file = find_dotenv()\n",
    "if env_file:\n",
    "    load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key is None:\n",
    "    openai_api_key = input('Please, enter your OpenAI API key: ')\n",
    "\n",
    "parser = ParserAI(openai_api_key=openai_api_key)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65a30490ab058365",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 2 experiments in this doc:\n",
    "1. [List of YCombinator companies](https://www.ycombinator.com/companies/)\n",
    "2. [List of commits in the repository](https://github.com/scraperai/scraperai/commits/main/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b4399c94da6d0c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiment 1. List of YCombinator companies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aac5b0bd2dcc9f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Open the website page\n",
    "Later, in case of multiple similar sites you will be able to run batch scraping. The main target is to semi-automatically detect all xpaths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b3f41887ec2da8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "url = 'https://www.ycombinator.com/companies' # Enter the URL of the website"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6487f025928429c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Open page in the browser\n",
    "crawler.get(url)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf21beb3647ca5b2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3.1. Detect page type\n",
    "We divide webpages into 4 categories:\n",
    "- **Catalog**: consists of similar-looking repeating elements. It can be a list of products, articles, companies, table rows, etc;\n",
    "- **Details**: contains main information about one product;\n",
    "- **Captcha**: in case we meet anti-scraping CAPTCHA;\n",
    "- **Other**: everything else; we don't support this webpage type yet.\n",
    "\n",
    "By default, we use screenshot of the page and GPT4 Vision model to determine a type. We also have a fallback algorithm if you cannot take a screenshot of the page or do not have access to Vision models.\n",
    "\n",
    "If you know the type of the page, you can set it manually."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49fc7fe1abe83b3d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "page_type = parser.detect_page_type(\n",
    "    page_source=crawler.page_source,\n",
    "    screenshot=crawler.get_screenshot_as_base64()\n",
    ")\n",
    "# You can set type manually\n",
    "# page_type = WebpageType.CATALOG\n",
    "page_type"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3848286a63d28b49",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "OpenAI tokens are spent on each action. You can find total money spent using:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser.total_cost  # in USD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3.2. Detect pagination\n",
    "**It is used only for type `catalog`.**\n",
    "\n",
    "We need to pass a whole page to detect the pagination.\n",
    "There are 3 types of pagination: `xpath`, `scroll`, and `url_param` (not implemented yet)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9124a022c312464c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pagination = parser.detect_pagination(crawler.page_source)\n",
    "pagination"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "418fe9a21ca498a7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In case of error, you can set it manually."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scroll type\n",
    "p1 = Pagination(type='scroll')\n",
    "# XPATH\n",
    "p2 = Pagination(type='xpath', xpath='//some-xpath')\n",
    "# URL param\n",
    "p3 = Pagination(type='url_param', url_param='page')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3.3. Detect catalog items\n",
    "**It is used only for type `catalog`.**\n",
    "You should correctly choose item block, url and ... \n",
    "AI isn't perfect, so you can manually add extra prompt to help AI to understand what you want or set xpath manually."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee342c3eb6a3175d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "catalog_item = parser.detect_catalog_item(page_source=crawler.page_source, website_url=url, extra_prompt=None)\n",
    "catalog_item"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7e88c3f7c70211b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can highlight fields using selenium:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if catalog_item is not None:\n",
    "    crawler.highlight_by_xpath(catalog_item.card_xpath, '#8981D7', 5)\n",
    "    crawler.highlight_by_xpath(catalog_item.url_xpath, '#5499D1', 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3.4. Detect data fields in a catalog item\n",
    "\n",
    "We define two types of data fields in a HTML page.\n",
    "\n",
    "First type is static field that do not contain a field name. It can be both a single value or an array. Example: product name or price.\n",
    "\n",
    "Second type is dynamic fields where there are both field names and values mentioned. Usually these fields look like tables:\n",
    "param1: value1\n",
    "param2: value2\n",
    "etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Aux method to print detected fields\n",
    "def _print_fields(fields: WebpageFields):\n",
    "    print(f'Static fields ({len(fields.static_fields)}):')\n",
    "\n",
    "    data = [{'name': f.field_name, 'xpath': f.field_xpath, 'value': f.first_value} for f in fields.static_fields]\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.to_markdown(tablefmt='plain', index=True))\n",
    "\n",
    "    print()\n",
    "    print(f'Dynamic fields ({len(fields.dynamic_fields)}):')\n",
    "    if len(fields.dynamic_fields) == 0:\n",
    "        print('Not found')\n",
    "        return\n",
    "    index = len(fields.static_fields)\n",
    "    for field in fields.dynamic_fields:\n",
    "        print(f' {index}  Section {field.section_name}\\n'\n",
    "                   f'    Labels xpath: {field.name_xpath}\\n'\n",
    "                   f'    Values xpath: {field.value_xpath}\\n'\n",
    "                   f'    Value: {field.first_values}')\n",
    "        index += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fields = parser.extract_fields(html_snippet=catalog_item.html_snippet)\n",
    "_print_fields(fields)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61b05e7bab0a9043",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can highlight detected fields:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Method to highlight fields\n",
    "def highlight_fields(fields: WebpageFields):\n",
    "    colors = ['#539878', '#5499D1', '#549B9A', '#5982A3', '#5A5499', '#68D5A2', '#75DDDC', '#8981D7', '#98D1FF',\n",
    "              '#98FFCF', '#9D5A5A', '#A05789', '#AAFFFE', '#C6C1FF', '#CD7CB3', '#D17A79', '#FAB4E4', '#FFB1B0']\n",
    "    for index, field in enumerate(fields.static_fields):\n",
    "        crawler.highlight_by_xpath(field.field_xpath, colors[index % len(colors)], border=4)\n",
    "    for index, field in enumerate(fields.dynamic_fields):\n",
    "        color = colors[index % len(colors)]\n",
    "        crawler.highlight_by_xpath(field.value_xpath, color, border=3)\n",
    "        crawler.highlight_by_xpath(field.name_xpath, color, border=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78308dd8b41d151a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "highlight_fields(fields)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3.4 Scrape data\n",
    "\n",
    "We are almost there!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, let's set some limits for simplicity:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_pages = 5  # How many catalog pages we should iterate over\n",
    "max_rows = 200  # How many rows to scrape before stop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's init scraper config that you can reuse later:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = ScraperConfig(\n",
    "    start_url=url,\n",
    "    page_type=page_type,\n",
    "    pagination=pagination,\n",
    "    catalog_item=catalog_item,\n",
    "    open_nested_pages=False,\n",
    "    fields=fields,\n",
    "    max_pages=max_pages,\n",
    "    max_rows=max_rows\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to create a Scraper instance using Crawler and ScraperConfig. It will iterate over catalog cards, automatically handle pagination and data-extracting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scraper = Scraper(config, crawler)\n",
    "\n",
    "rows = []\n",
    "for item in tqdm(scraper.scrape(), total=max_rows):\n",
    "    rows.append(item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Congratulations! We got the final data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can export data in any format:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export as json\n",
    "with open('yc.json', 'w+') as f:\n",
    "    json.dump(rows, f, indent=4)\n",
    "\n",
    "# Export to Pandas DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv('yc.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. Parse nested detail page\n",
    "\n",
    "You can extract data from nested pages using ScraperAI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Open first nested page\n",
    "crawler.get(catalog_item.urls_on_page[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4.1. Extract fields\n",
    "\n",
    "First, we use `summarize_details_page_as_valid_html` method to find relevant parts on the initial webpage.\n",
    "For example, a list of similar products is not a relevant part of a details page.\n",
    "\n",
    "Then we use `parser.extract_fields` as before to get the fields from html snippet."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "html_snippet = parser.summarize_details_page_as_valid_html(\n",
    "    page_source=crawler.page_source,\n",
    "    screenshot=crawler.get_screenshot_as_base64()\n",
    ")\n",
    "fields = parser.extract_fields(html_snippet)\n",
    "_print_fields(fields)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's highlight the fields\n",
    "highlight_fields(fields)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4.2. Scrape data\n",
    "\n",
    "First, let's set some limits for simplicity:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_pages = 5  # How many catalog pages we should iterate over\n",
    "max_rows = 20  # How many rows to scrape before stop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are ready to collect the data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = ScraperConfig(\n",
    "    start_url=url,\n",
    "    page_type=page_type,\n",
    "    pagination=pagination,\n",
    "    catalog_item=catalog_item,\n",
    "    open_nested_pages=True,\n",
    "    fields=fields,\n",
    "    max_pages=max_pages,\n",
    "    max_rows=max_rows\n",
    ")\n",
    "scraper = Scraper(config, crawler)\n",
    "\n",
    "rows = []\n",
    "for item in tqdm(scraper.scrape(), total=max_rows):\n",
    "    rows.append(item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(rows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiment 2. [List of commits in the repository](https://github.com/scraperai/scraperai/commits/main/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define url\n",
    "url = 'https://github.com/scraperai/scraperai/commits/main/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Open url\n",
    "crawler.get(url)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detect page_type\n",
    "page_type = WebpageType.CATALOG\n",
    "page_type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detect pagination\n",
    "pagination = parser.detect_pagination(crawler.page_source)\n",
    "pagination"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detect catalog item\n",
    "catalog_item = parser.detect_catalog_item(\n",
    "    page_source=crawler.page_source,\n",
    "    website_url=url,\n",
    "    extra_prompt='This page contains a list of commits. Each commit row is a catalog item')\n",
    "catalog_item"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crawler.highlight_by_xpath(catalog_item.card_xpath, '#8981D7', 5)\n",
    "crawler.highlight_by_xpath(catalog_item.url_xpath, '#5499D1', 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fields = parser.extract_fields(html_snippet=catalog_item.html_snippet)\n",
    "_print_fields(fields)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "highlight_fields(fields)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = ScraperConfig(\n",
    "    start_url=url,\n",
    "    page_type=page_type,\n",
    "    pagination=pagination,\n",
    "    catalog_item=catalog_item,\n",
    "    open_nested_pages=True,\n",
    "    fields=fields,\n",
    "    max_pages=2,\n",
    "    max_rows=100\n",
    ")\n",
    "scraper = Scraper(config, crawler)\n",
    "\n",
    "rows = []\n",
    "for item in tqdm(scraper.scrape(), total=max_rows):\n",
    "    rows.append(item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(rows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### That's it!\n",
    "#### Thank you for using ScraperAI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
